{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { margin-left: 0 !important; width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import cmath\n",
    "import glob\n",
    "import itertools\n",
    "import tensorboard\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example0:\n",
    "# {\n",
    "#   \"EntryType\":\"EquivalentLoudnessMeasurement\",\n",
    "#   \"Calibration\":{\n",
    "#     \"HeadphoneFrequencyResponseHash\":\"eab807a5050b0087109ac1fd6fecbd09197a3dcd\",\n",
    "#     \"FullScaleSineDBSPL\":100\n",
    "#   },\n",
    "#   \"Run\":{\n",
    "#     \"ID\":\"1599737802659_2857050667\"\n",
    "#   },\n",
    "#   \"Evaluation\":{\n",
    "#     \"ID\":\"1599738237070_2365796582\",\n",
    "#     \"Frequency\":697.9866246810275,\n",
    "#     \"Probe\":{\n",
    "#       \"Type\":\"Noise\",\n",
    "#       \"Params\":{\n",
    "#         \"Level\":-10,\n",
    "#         \"LowerLimit\":966.84025,\n",
    "#         \"Onset\":{\n",
    "#           \"Delay\":0,\n",
    "#           \"Duration\":0.1\n",
    "#         },\n",
    "#         \"UpperLimit\":1033.15975\n",
    "#       }\n",
    "#     },\n",
    "#     \"Combined\":{\n",
    "#       \"Type\":\"Superposition\",\n",
    "#       \"Params\":[\n",
    "#         {\n",
    "#           \"Params\":{\n",
    "#             \"Level\":-40,\n",
    "#             \"LowerLimit\":966.84025,\n",
    "#             \"Onset\":{\n",
    "#               \"Delay\":0.5,\n",
    "#               \"Duration\":0.1\n",
    "#             },\n",
    "#             \"UpperLimit\":1033.15975\n",
    "#           },\n",
    "#           \"Type\":\"Noise\"\n",
    "#         },{\n",
    "#           \"Params\":{\n",
    "#             \"Level\":-20,\n",
    "#             \"LowerLimit\":672.9766301106662,\n",
    "#             \"Onset\":{\n",
    "#               \"Delay\":0,\n",
    "#               \"Duration\":0.1\n",
    "#             },\n",
    "#             \"UpperLimit\":722.9966192513888\n",
    "#           },\n",
    "#           \"Type\":\"Noise\"\n",
    "#         }\n",
    "#       ]\n",
    "#     }\n",
    "#   },\n",
    "#   \"Results\":{\n",
    "#     \"ProbeGainForEquivalentLoudness\":0.003107877399956887,\n",
    "#     \"ProbeDBSPLForEquivalentLoudness\":39.84927756697657\n",
    "#   }\n",
    "# }\n",
    "def experiment_result_to_example(exp, car_fs_sine_db_spl=100, window_size=2048, sample_rate=48000, noise_floor_db_spl=30):\n",
    "    bin_width = sample_rate / window_size\n",
    "    if exp.EntryType != 'EquivalentLoudnessMeasurement':\n",
    "        return None\n",
    "    assert exp.Evaluation.Probe.Type == 'Noise'\n",
    "    assert exp.Evaluation.Combined.Type == 'Superposition'\n",
    "    for part in exp.Evaluation.Combined.Params:\n",
    "        assert part.Type == 'Noise'\n",
    "        # If this part isn't the probe.\n",
    "        if (part.Params.LowerLimit != exp.Evaluation.Probe.Params.LowerLimit or\n",
    "            part.Params.UpperLimit != exp.Evaluation.Probe.Params.UpperLimit):\n",
    "            # We want to ignore evaluations where probe and masker overlap.\n",
    "            if ((part.Params.LowerLimit + bin_width * 2 >= exp.Evaluation.Probe.Params.LowerLimit and\n",
    "                 part.Params.LowerLimit - bin_width * 2 <= exp.Evaluation.Probe.Params.UpperLimit) or\n",
    "                (part.Params.UpperLimit + bin_width * 2 >= exp.Evaluation.Probe.Params.LowerLimit and\n",
    "                 part.Params.UpperLimit - bin_width * 2 <= exp.Evaluation.Probe.Params.UpperLimit) or\n",
    "                (part.Params.LowerLimit - bin_width * 2 <= exp.Evaluation.Probe.Params.LowerLimit and\n",
    "                 part.Params.UpperLimit + bin_width * 2 >= exp.Evaluation.Probe.Params.UpperLimit)):\n",
    "                return None\n",
    "    assert exp.Results.ProbeGainForEquivalentLoudness > 0\n",
    "    assert exp.Results.ProbeDBSPLForEquivalentLoudness != None\n",
    "    assert exp.Calibration.FullScaleSineDBSPL > 0\n",
    "\n",
    "    probe_fs = [exp.Evaluation.Probe.Params.LowerLimit, exp.Evaluation.Probe.Params.UpperLimit]\n",
    "    combined_sigs = []\n",
    "    for part in exp.Evaluation.Combined.Params:\n",
    "        part_db_spl = exp.Calibration.FullScaleSineDBSPL + part.Params.Level\n",
    "        part_db_fs = part_db_spl - car_fs_sine_db_spl\n",
    "        combined_sigs.append([part_db_fs, part.Params.LowerLimit, part.Params.UpperLimit])\n",
    "    return synthesize_example(probe_fs, exp.Results.ProbeDBSPLForEquivalentLoudness, combined_sigs, noise_floor_db_spl - car_fs_sine_db_spl, window_size=window_size, sample_rate=sample_rate)\n",
    "\n",
    "def make_noise(total_power_db_fs, lower_limit, upper_limit, window_size=2048, sample_rate=48000):\n",
    "    np.random.seed(0)\n",
    "    # Power of full scale sine is variance, which is 0.5\n",
    "    fs_offset = -10 * np.log10(0.5)\n",
    "    bin_width = sample_rate / window_size\n",
    "    first_bin = int(np.floor(lower_limit/bin_width))\n",
    "    last_bin = int(np.ceil(upper_limit/bin_width))\n",
    "    noise_coeffs = np.zeros(shape=[window_size//2], dtype=np.complex128)\n",
    "    noise_coeffs[first_bin:last_bin+1] = (complex(0, 1) * np.random.normal(size=[last_bin-first_bin+1]) + np.random.normal(size=[last_bin-first_bin+1]))\n",
    "    noise_coeffs = np.concatenate([noise_coeffs, [0], np.flip(np.conj(noise_coeffs[1:]))])\n",
    "    noise_signal = np.real(np.fft.ifft(noise_coeffs))\n",
    "    noise_power_db_fs = 10 * np.log10(np.var(noise_signal)) + fs_offset\n",
    "    scale = 10 ** ((total_power_db_fs - noise_power_db_fs) / 20)\n",
    "    noise_signal *= scale\n",
    "    new_power = 10 * np.log10(np.var(noise_signal)) + fs_offset\n",
    "    if np.abs(new_power - total_power_db_fs) > 1e-6:\n",
    "        raise ValueError(f'wrong power, got {new_power} wanted {total_power_db_fs}')\n",
    "    return noise_signal\n",
    "\n",
    "# probe_fs are [lowest_freq_hz, highest_freq_hz]\n",
    "# combined_sigs are [[intensity_db_fs, lowest_freq_hz, highest_freq_hz]]\n",
    "def synthesize_example(probe_fs, equiv_intensity_db_fs, combined_sigs, noise_floor_db_fs, window_size=2048, sample_rate=48000):\n",
    "    bin_width = sample_rate / window_size\n",
    "\n",
    "    signal = make_noise(noise_floor_db_fs, 20, 14000, window_size=window_size)\n",
    "    \n",
    "    for part in combined_sigs:\n",
    "        signal += make_noise(part[0], part[1], part[2], window_size=window_size)\n",
    "    \n",
    "    relevant_bins = np.zeros([int(window_size/2)], dtype=np.float64)\n",
    "    first_relevant_bin = int(np.floor(probe_fs[0]/bin_width))\n",
    "    last_relevant_bin = int(np.ceil(probe_fs[1]/bin_width))\n",
    "    relevant_bins[first_relevant_bin:last_relevant_bin+1] = 1\n",
    "\n",
    "    true_loudness = np.array([equiv_intensity_db_fs], dtype=np.float64)\n",
    "    \n",
    "    res = tf.concat([signal, true_loudness, relevant_bins], axis=0)\n",
    "    return res[:,None]\n",
    "\n",
    "def prepare_examples(examples):\n",
    "    filtered_iter = filter(lambda e: e != None, examples)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(list(filtered_iter))\n",
    "    input_true_ds = ds.map(lambda ex: (ex[:window_size], ex[window_size:]))\n",
    "    return input_true_ds\n",
    "\n",
    "def load_examples(glb, car_fs_sine_db_spl=100, window_size=2048, sample_rate=48000):\n",
    "    lines_iter = itertools.chain(*map(lambda f: open(f).readlines(), glob.glob(glb)))\n",
    "    json_iter = map(lambda l: json.loads(l, object_hook=lambda o: SimpleNamespace(**o)), lines_iter)\n",
    "    only_eval_iter = filter(lambda exp: exp.EntryType == 'EquivalentLoudnessMeasurement', json_iter)\n",
    "    sorted_iter = sorted(only_eval_iter, key=lambda exp: exp.Evaluation.ID)\n",
    "    examples_iter = map(lambda l: experiment_result_to_example(l, car_fs_sine_db_spl=car_fs_sine_db_spl, window_size=window_size, sample_rate=sample_rate), sorted_iter)\n",
    "    return prepare_examples(examples_iter)\n",
    "            \n",
    "def plot_pz(f):\n",
    "    _, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.add_patch(patches.Circle((0,0),\n",
    "                              radius=1,\n",
    "                              fill=False,\n",
    "                              color='black',\n",
    "                              ls='solid',\n",
    "                              alpha=0.1))\n",
    "    ax.axvline(0, color='0.7')\n",
    "    ax.axhline(0, color='0.7')\n",
    "    ax.set_xlim((-1.1,1.1))\n",
    "    ax.set_ylim((-1.1,1.1))\n",
    "\n",
    "    ax.plot(tf.reshape(tf.math.real(f.poles), [-1,1]), tf.reshape(tf.math.imag(f.poles), [-1,1]),\n",
    "            'x', markersize=9, alpha=0.5)\n",
    "    ax.plot(tf.reshape(tf.math.real(f.zeros), [-1,1]), tf.reshape(tf.math.imag(f.zeros), [-1,1]),\n",
    "            'o', color='none', markeredgecolor='red',\n",
    "            markersize=9, alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_many_z(many_z, sample_rate=48000, figsize=(12,4)):\n",
    "    xaxis = np.tile(np.linspace(0,\n",
    "                                (many_z.shape[1] - 1) * sample_rate *\n",
    "                                0.5 / many_z.shape[1],\n",
    "                                many_z.shape[1]), [many_z.shape[0], 1])\n",
    "    _, ax = plt.subplots(figsize=figsize)    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim((10,20000))\n",
    "    ax.set_ylim((-20, 70))\n",
    "    x = np.transpose(xaxis[:,:xaxis.shape[1]//2])\n",
    "    y = np.transpose(20 * np.log10(1e-20+np.abs(many_z[:,:many_z.shape[1]//2])))\n",
    "    ax.plot(x, y)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "def plot_snr(snr, channels=[None, None], bins=[None, None]):\n",
    "    max_snr = np.nanmax(snr)\n",
    "    min_snr = np.nanmin(snr)\n",
    "    scaled_snr = ((snr - min_snr) / (max_snr - min_snr))[:,:,None]\n",
    "    if channels[1] != None:\n",
    "        scaled_snr = scaled_snr[:channels[1]]\n",
    "    if channels[0] != None:\n",
    "        scaled_snr = scaled_snr[channels[0]]\n",
    "    if bins[1] != None:\n",
    "        scaled_snr= scaled_snr[:,:bins[1]]\n",
    "    if bins[0] != None:\n",
    "        scaled_snr = scaled_snr[:,bins[0]:]\n",
    "    scaled_snr = np.repeat(scaled_snr, 4, axis=0)\n",
    "    pixels = np.concatenate([scaled_snr, scaled_snr/2, scaled_snr/2], axis=2)\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.imshow(pixels, interpolation='bicubic')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pred(model, ds):\n",
    "    model = tf.function(model)\n",
    "    truth = []\n",
    "    pred = []\n",
    "    worst_example = None\n",
    "    worst_error = None\n",
    "    worst_index = None\n",
    "    example_index = 0\n",
    "    for exIdx, ex in enumerate(ds):\n",
    "        x, y_true = ex\n",
    "        y_true = tf.reshape(y_true, [y_true.shape[0], y_true.shape[1]])\n",
    "        for val in y_true[:,0]:\n",
    "            truth.append(np.abs(val))\n",
    "        y_pred = model(x)\n",
    "        pred_min = np.min(y_pred, axis=1)[:,None]\n",
    "        irrelevant_reduced = np.where(np.abs(y_true[:,1:]) != 0, y_pred, pred_min-1)\n",
    "        for idx, val in enumerate(np.max(irrelevant_reduced, axis=1)):\n",
    "            pred.append(val)\n",
    "            err = (truth[idx] - val) ** 2 * truth[idx]            \n",
    "            if worst_error is None or err > worst_error:\n",
    "                worst_error = err\n",
    "                worst_example = x[idx]\n",
    "                worst_index = example_index\n",
    "            example_index += 1\n",
    "    plt.figure(figsize=(10,5))\n",
    "    steps = np.arange(len(truth))\n",
    "    plt.plot(steps, truth, label='truth')\n",
    "    plt.plot(steps, pred, label='pred')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return (worst_index, worst_error, worst_example)\n",
    "\n",
    "# (a - x)*(b - x)*... => k1 + k2*x + k3*x^2 + ... \n",
    "# This function takes [a,b,...] and produces k1,k2,k3,...\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def coeffs(constants, length=None):\n",
    "    if length is None:\n",
    "        length = constants.shape[0]\n",
    "    res = tf.TensorArray(size=length+1, dtype=tf.complex128)\n",
    "    for num in range(length+1):\n",
    "        if num == 0:\n",
    "            s = tf.constant(1.0, dtype=tf.complex128)\n",
    "        else:\n",
    "            s = tf.constant(0.0, dtype=tf.complex128)\n",
    "            for parts in itertools.combinations(np.arange(length), num):\n",
    "                prod = tf.constant(1.0, dtype=tf.complex128)\n",
    "                for part in parts:\n",
    "                    prod *= -constants[part]\n",
    "                s += prod\n",
    "        res = res.write(num, s)\n",
    "    return res.stack()\n",
    "\n",
    "class RNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cell, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cell = cell\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        state = tuple(tf.zeros(shape=[inputs.shape[0], size], dtype=self.dtype) for size in self.cell.state_size)\n",
    "        def cond(step, output, state):\n",
    "            return step < inputs.shape[1]\n",
    "        def body(step, output, state):\n",
    "            step_input = inputs[:,step]\n",
    "            step_output, state = self.cell.call(step_input, state)\n",
    "            output = output.write(step, step_output)\n",
    "            return step+1, output, state\n",
    "        _, output, self.state = tf.while_loop(\n",
    "            shape_invariants=[\n",
    "                tf.TensorShape(None), \n",
    "                tf.TensorShape(None), \n",
    "                tuple([s.shape for s in state])\n",
    "            ],\n",
    "            cond=cond, \n",
    "            body=body, \n",
    "            loop_vars=[\n",
    "                0, \n",
    "                tf.TensorArray(size=inputs.shape[1], dtype=inputs.dtype), \n",
    "                state\n",
    "            ],\n",
    "        )\n",
    "        output = output.stack()\n",
    "        dims = np.arange(len(output.shape))\n",
    "        dims[0] = 1\n",
    "        dims[1] = 0\n",
    "        output = tf.transpose(output, dims)\n",
    "        return output\n",
    "\n",
    "class PZCell:\n",
    "    def __init__(self, gain, poles, zeros):\n",
    "        self.gain = gain\n",
    "        self.poles = poles\n",
    "        self._n_poles = poles.shape[0] * 2\n",
    "        self.zeros = zeros\n",
    "        self._n_zeros = zeros.shape[0] * 2\n",
    "        self.output_size = 1\n",
    "        self.state_size = [2*self.poles.shape[0]+1, 2*self.poles.shape[0]+1]\n",
    "    # [batch, 1], ([batch, steps_back], [batch, steps_back]) - 0 is at t-1 => [batch, 1], ([batch, steps_back], [batch, steps_back]) - 0 is at t\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, input_at_t, states_at_t):\n",
    "        print('pzc input, state', input_at_t, states_at_t)\n",
    "        input_dtype = input_at_t.dtype\n",
    "        states_dtype = states_at_t[0].dtype\n",
    "        poles = tf.reshape(tf.concat([self.poles, tf.math.conj(self.poles)], axis=0), [self._n_poles])\n",
    "        zeros = tf.reshape(tf.concat([self.zeros, tf.math.conj(self.zeros)], axis=0), [self._n_zeros])\n",
    "        input_at_t = tf.cast(input_at_t, dtype=tf.complex128)\n",
    "        x_memory = tf.cast(states_at_t[0], tf.complex128)\n",
    "        y_memory = tf.cast(states_at_t[1], tf.complex128)\n",
    "        x_memory = tf.concat([input_at_t, x_memory[:, :x_memory.shape[1]-1]], axis=1)\n",
    "        pole_coeffs = coeffs(poles, self._n_poles)\n",
    "        zero_coeffs = coeffs(zeros, self._n_zeros)\n",
    "        zero_offset = tf.math.maximum(0, self._n_poles - self._n_zeros)\n",
    "        output_at_t = tf.constant(0, shape=input_at_t.shape, dtype=tf.complex128)\n",
    "        zero_components = x_memory[:,zero_offset:] * self.gain * zero_coeffs\n",
    "        output_at_t += tf.math.reduce_sum(zero_components, axis=1)[:,None]\n",
    "        pole_components = y_memory[:,:y_memory.shape[1]-1] * pole_coeffs[1:]\n",
    "        output_at_t -= tf.math.reduce_sum(pole_components, axis=1)[:, None]\n",
    "        output_at_t = tf.math.divide_no_nan(output_at_t, pole_coeffs[0])\n",
    "        y_memory = tf.concat([output_at_t, y_memory[:, :y_memory.shape[1]-1]], axis=1)\n",
    "        states_at_t_plus_1 = (tf.cast(tf.math.real(x_memory), states_dtype), tf.cast(tf.math.real(y_memory), states_dtype))\n",
    "        output_at_t = tf.math.real(tf.reshape(output_at_t, [output_at_t.shape[0]]))\n",
    "        output_at_t = tf.cast(output_at_t[:,None], input_dtype)\n",
    "        #print('pzc output, state', output_at_t, states_at_t_plus_1)\n",
    "        return (output_at_t, states_at_t_plus_1)\n",
    "        \n",
    "class CARCell:\n",
    "    # See 'Human and Machine Hearing' (http://dicklyon.com/hmh/), 16.2-16.3,\n",
    "    # and https://github.com/google/carfac/.\n",
    "    def __init__(self, sample_rate=48000, erb_per_step=0.5, linear=False):\n",
    "        self._erb_per_step = erb_per_step\n",
    "        self._sample_rate = sample_rate\n",
    "        self._linear = linear\n",
    "\n",
    "        # Based on the assumtion that max small-signal gain at the passband peak\n",
    "        # will be on the order of (0.5/min_zeta)^(1/erb_per_step), and we need\n",
    "        # the start value of that in the same region or the loss function becomes\n",
    "        # too uneven to optimize.\n",
    "        def compute_zeta(zeta_at_default_erb_per_step, erb_per_step):\n",
    "            default_erb_per_step = 0.5\n",
    "            max_small_signal_gain = (0.5 /\n",
    "                                     zeta_at_default_erb_per_step) ** (1 / default_erb_per_step)\n",
    "            return 0.5 / (max_small_signal_gain ** erb_per_step)\n",
    "\n",
    "        # Controls r (pole and zero abs value) which controls damping relative to\n",
    "        # frequency.\n",
    "        self._high_f_damping_compression = tf.Variable(\n",
    "            name='high_f_damping_compression',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=0.5)\n",
    "        # Controls distance from pole to zero.\n",
    "        self._zero_ratio = tf.Variable(\n",
    "            name='zero_ratio',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=(2 ** 0.5))\n",
    "        # min/max zeta controls max damping.\n",
    "        self._min_zeta = tf.Variable(\n",
    "            name='min_zeta',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=compute_zeta(0.1, self._erb_per_step))\n",
    "        self._max_zeta = tf.Variable(\n",
    "            name='max_zeta',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=compute_zeta(0.35, self._erb_per_step))\n",
    "        # Controls how we convert from Hz to Cams.\n",
    "        # Defaults at numbers from B.C.J. Moore and B.R. Glasberg, \"Suggested formulae for calculating auditory-filter bandwidths and excitation patterns\".\n",
    "        self._erb_constant_0 = tf.Variable(\n",
    "            name='erb_constant_0(24.7)',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=24.7)\n",
    "        self._erb_constant_1 = tf.Variable(\n",
    "            name='erb_constant_1(1.0)',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=1)\n",
    "        self._erb_constant_2 = tf.Variable(\n",
    "            name='erb_constant_2(4.37)',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=4.37)\n",
    "        self._v_offset = tf.Variable(\n",
    "            name='v_offset',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=0.04)\n",
    "        self._velocity_scale = tf.Variable(\n",
    "            name='velocity_scale',\n",
    "            dtype=tf.float64,\n",
    "            initial_value=0.1)\n",
    "        self._max_freq = 20000\n",
    "        self._min_freq = 20\n",
    "        f = self._max_freq\n",
    "        self._n_poles = 1\n",
    "        while f - self._erb_per_step * self.ERB(f) > self._min_freq:\n",
    "            f = f - self._erb_per_step * self.ERB(f)\n",
    "            self._n_poles += 1\n",
    "            \n",
    "        self.output_size = self._n_poles\n",
    "        # [z1, z2]\n",
    "        self.state_size = [self.output_size, self.output_size, self.output_size]\n",
    "\n",
    "    # Equivalent rectangular bandwidth, the width of the theoretical rectangular cochlear place filter.\n",
    "    def ERB(self, f):\n",
    "        return self._erb_constant_0 * (self._erb_constant_1 + self._erb_constant_2 * f * 0.001)\n",
    "    # [batch, 1], ([batch, prev_u], [batch, prev_v]) => [batch, channel], ([batch, u], [batch, v])\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, input_at_t, states_at_t):\n",
    "        #print('cc call input, state', input_at_t, states_at_t)\n",
    "        input_dtype = input_at_t.dtype\n",
    "        states_dtype = states_at_t[0].dtype\n",
    "        \n",
    "        input_at_t = tf.cast(input_at_t, dtype=tf.float64)\n",
    "\n",
    "        pole_freqs_ta = tf.TensorArray(size=self._n_poles, dtype=tf.float64)\n",
    "        f = self._max_freq\n",
    "        for channel_number in range(self._n_poles):\n",
    "            pole_freqs_ta = pole_freqs_ta.write(channel_number, f)\n",
    "            f -= self._erb_per_step * self.ERB(f)\n",
    "        pole_freqs = pole_freqs_ta.stack()\n",
    "        #print('cc call pole_freqs', pole_freqs.shape)\n",
    "\n",
    "        # From the matlab code:\n",
    "        # zero_ratio comes in via h.  In book's circuit D, zero_ratio is 1/sqrt(a),\n",
    "        # and that a is here 1 / (1+f) where h = f*c.\n",
    "        # solve for f:  1/zero_ratio^2 = 1 / (1+f)\n",
    "        # zero_ratio^2 = 1+f => f = zero_ratio^2 - 1\n",
    "        f = tf.math.square(self._zero_ratio) - 1\n",
    "        pole_thetas = pole_freqs * 2 * np.pi / self._sample_rate\n",
    "        # The book assigns a0 and c0 thus to simplify the equations.\n",
    "        a0 = tf.math.cos(pole_thetas)\n",
    "        c0 = tf.math.sin(pole_thetas)\n",
    "        #print('cc call a0 c0', a0.shape, c0.shape)\n",
    "\n",
    "        # The ratio between each pole and max measurable frequency.\n",
    "        x = pole_thetas / np.pi\n",
    "\n",
    "        # From the matlab code:\n",
    "        # When high_f_damping_compression is 0 this is just theta, when\n",
    "        # high_f_damping_compression is 1 it approaches 0 as theta approaches pi.\n",
    "        zr_coeffs = np.pi * (x - self._high_f_damping_compression * tf.math.pow(x, 3))\n",
    "\n",
    "        # The book is not super easy to follow here, so I have mostly\n",
    "        # implemented the same math as the matlab and c++ code.\n",
    "\n",
    "        r1 = (1 - zr_coeffs * self._max_zeta)\n",
    "        min_zetas = self._min_zeta + (0.25 * ((self.ERB(pole_freqs) / pole_freqs) - self._min_zeta))\n",
    "        zr_coeffs *= (self._max_zeta - min_zetas)\n",
    "\n",
    "        prev_u = tf.cast(states_at_t[0], tf.float64)\n",
    "        prev_v = tf.cast(states_at_t[1], tf.float64)\n",
    "        # This is called zA_memory in the matlab code, its a delay of prev_v.\n",
    "        prev_vel_mem = tf.cast(states_at_t[2], tf.float64)\n",
    "\n",
    "        # Velocity is the velocity of the pressure differential.\n",
    "        velocity = prev_v - prev_vel_mem\n",
    "        #print('car velocity', velocity.shape)\n",
    "        # NLF is the nonlinearity doing the fast acting compression.\n",
    "        nlf_out = 1.0 / (1.0 + tf.math.square(velocity * self._velocity_scale + self._v_offset))\n",
    "        #print('car nlf_out', nlf_out.shape)\n",
    "        # The undampening is controlled via the NLF.\n",
    "        if self._linear:\n",
    "            r = r1 + zr_coeffs * tf.ones(shape=velocity.shape, dtype=tf.float64)\n",
    "        else:\n",
    "            r = r1 + zr_coeffs * nlf_out\n",
    "        #print('car r', r.shape)\n",
    "\n",
    "        h = c0 * f\n",
    "        g0 = (1 - 2 * r * a0 + tf.math.square(r)) / (1 - 2 * r * a0 + h * r * c0 + tf.math.square(r))\n",
    "        \n",
    "        # Following is based on matlab code and figures 16.1 and 17.1 from the book. In figure 17.1\n",
    "        # I have used the names U and V in the same way as in figure 16.1.\n",
    "        \n",
    "        #print('cc call r, prev_u, prev_v', r.shape, prev_u.shape, prev_v.shape)\n",
    "        # This doesn't include the input X\n",
    "        partial_u = r * (a0 * prev_u - c0 * prev_v)\n",
    "        # V doesn't include the input \n",
    "        v = r * (c0 * prev_u + a0 * prev_v)\n",
    "        #print('cc call v, partial_u', v.shape, partial_u.shape)\n",
    "        \n",
    "        # This doesn't include the input X\n",
    "        partial_y = h * v\n",
    "        #print('cc call partial_y, g0', partial_y.shape, g0.shape)\n",
    "\n",
    "        u_ta = tf.TensorArray(size=self._n_poles, dtype=tf.float64)\n",
    "        output_at_t_ta = tf.TensorArray(size=self._n_poles, dtype=tf.float64)\n",
    "        in_out = input_at_t[:,0]\n",
    "        for ch in range(self._n_poles):\n",
    "            #print('cc call partial_u[:,ch], in_out', partial_u[:,ch].shape, in_out.shape)\n",
    "            # Add this input to partial_u to get actual u.\n",
    "            u_ta = u_ta.write(ch, partial_u[:,ch] + in_out)\n",
    "            # Add this input to partial_output to get actual output.\n",
    "            in_out = tf.math.real(g0[:,ch]) * (in_out + partial_y[:, ch])\n",
    "            # Save this actual output.\n",
    "            output_at_t_ta = output_at_t_ta.write(ch, in_out)\n",
    "\n",
    "        u = u_ta.stack()\n",
    "        #print('cc call u', u.shape)\n",
    "        u = tf.transpose(u, [1,0])\n",
    "        #print('cc call u', u.shape)\n",
    "        states_at_t_plus_1 = (tf.cast(u, states_dtype), tf.cast(v, states_dtype), tf.cast(prev_v, states_dtype))\n",
    "        output_at_t = output_at_t_ta.stack()\n",
    "        output_at_t = tf.cast(tf.transpose(output_at_t, [1,0]), input_dtype)\n",
    "        #print('cc call output, states', output_at_t, states_at_t_plus_1)\n",
    "        return (output_at_t, states_at_t_plus_1)\n",
    "    \n",
    "class FFTLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.math.abs(tf.signal.fft(tf.cast(inputs, tf.complex128)))\n",
    "    \n",
    "class SNRLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    # [...,fft_coeffs] => [...,channels,db_snr]\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        gain = tf.math.abs(inputs)\n",
    "        signal_power = tf.math.square(gain)\n",
    "        noise_power = tf.math.reduce_sum(signal_power, axis=2)[:,:,None] - signal_power\n",
    "        snr = tf.math.divide_no_nan(tf.math.square(signal_power), noise_power)\n",
    "        res = tf.cast(10.0, self.dtype) * tf.cast(tf.math.log(snr + 1e-16), self.dtype) / tf.cast(tf.math.log(10.0), self.dtype)\n",
    "        res = res[:,:,:res.shape[2]//2]\n",
    "        return res\n",
    "\n",
    "class LoudnessPredictorLoss(tf.keras.losses.Loss):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, y_true, y_pred):\n",
    "        true_partial_loudness = y_true[:,0,0]\n",
    "        relevant_bins = tf.cast(y_true[:,1:,0], tf.bool)\n",
    "        min_psnr = tf.math.reduce_min(y_pred, axis=1)[:,None]\n",
    "        reduced_irrelevant_bins = tf.where(relevant_bins, y_pred, min_psnr - 1)\n",
    "        psnr_across_relevant_bins = tf.math.reduce_max(reduced_irrelevant_bins, axis=1)\n",
    "        res = tf.cast(tf.keras.losses.MSE(true_partial_loudness, psnr_across_relevant_bins), true_partial_loudness.dtype) * true_partial_loudness\n",
    "        return res\n",
    "\n",
    "class LoudnessPredictorLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._loudness_offset = self.add_weight(\n",
    "            name='loudness_offset',\n",
    "            initializer=tf.keras.initializers.Constant(1),\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "        self._loudness_scale = self.add_weight(\n",
    "            name='loudness_scale',\n",
    "            initializer=tf.keras.initializers.Constant(1),\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "    # [...,fft_bin] => [...,equivalent_db_spl]\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        psnr_per_bin = tf.math.reduce_max(inputs, axis=1)\n",
    "        res = self._loudness_offset * -110 + self._loudness_scale *3 * psnr_per_bin\n",
    "        return res\n",
    "\n",
    "class EarLayer(RNNLayer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.gain = tf.Variable(initial_value=1.34, dtype=tf.complex128)\n",
    "        self.poles = tf.Variable(initial_value=[\n",
    "                     (-0.05429768147702485+1.4172655611120915e-05j),\n",
    "                     (0.2917622403739163+0.7731812636894612j),\n",
    "                     (0.8768382244780407-0.31120458350060115j),\n",
    "                     (0.6598943546882394-0.46728573398560225j)\n",
    "                 ], dtype=tf.complex128)\n",
    "        self.zeros = tf.Variable(initial_value=[\n",
    "                     (0.635496172349615+0.14499945287904842j),\n",
    "                     (0.30987058966944614-0.8574194617385421j),\n",
    "                     (0.5721096307971768-2.2915816453724273e-05j)\n",
    "                 ], dtype=tf.complex128)\n",
    "        cell = PZCell(self.gain, self.poles, self.zeros)\n",
    "        super().__init__(cell, **kwargs)\n",
    "        \n",
    "class CARLayer(RNNLayer):\n",
    "    def __init__(self, linear=False, erb_per_step=0.5, **kwargs):\n",
    "        self.cc = CARCell(linear=linear, erb_per_step=erb_per_step)\n",
    "        super().__init__(self.cc, **kwargs)\n",
    "        \n",
    "class SNRModel(tf.keras.Sequential):    \n",
    "    def __init__(self, linear=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ear_layer = EarLayer() \n",
    "        self.add(self.ear_layer)\n",
    "        self.car_layer = CARLayer(linear=linear)\n",
    "        self.add(self.car_layer)\n",
    "        self.add(tf.keras.layers.Permute((2,1)))\n",
    "        self.add(FFTLayer())\n",
    "        self.add(SNRLayer())\n",
    "    \n",
    "class LoudnessPredictorModel(SNRModel):\n",
    "    def __init__(self, linear=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.loudness_predictor_layer = LoudnessPredictorLayer()\n",
    "        self.add(self.loudness_predictor_layer)\n",
    "        \n",
    "def assertequal(a1, a2):\n",
    "    npa1 = np.array(a1)\n",
    "    npa2 = np.array(a2)\n",
    "    if npa1.shape != npa2.shape:\n",
    "        raise ValueError(f'a1 has shape {npa1.shape} and a2 has shape {npa2.shape}')\n",
    "    nonequal = (npa1 != npa2).nonzero()\n",
    "    nonequal1 = npa1[nonequal]\n",
    "    nonequal2 = npa2[nonequal]\n",
    "    if len(nonequal1) > 0:\n",
    "        raise ValueError(f'a1[{tuple(np.array(nonequal)[:,0])}] ({nonequal1[0]}) != a2{tuple(np.array(nonequal)[:,0])} ({nonequal2[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "bin_width = 48000 / window_size\n",
    "print(bin_width)\n",
    "mask_start = [80, 200]\n",
    "probe = [60, 1000]\n",
    "steps = 500\n",
    "width = 20\n",
    "examples = []\n",
    "overlap = [None, None]\n",
    "for idx, mask_center in enumerate(np.linspace(mask_start[1], probe[1] * 2, steps)):\n",
    "    if mask_center + width + bin_width >= probe[1] - width - bin_width and overlap[0] is None:\n",
    "        overlap[0] = idx\n",
    "    if overlap[0] is not None and overlap[1] is None and mask_center - width - bin_width > probe[1] + width + bin_width:\n",
    "        overlap[1] = idx\n",
    "    examples.append(synthesize_example([probe[1] - width, probe[1] + width], probe[0], [[probe[0], probe[1] - width, probe[1] + width], [mask_start[0], mask_center - width, mask_center + width]], -70))\n",
    "examples = prepare_examples(examples).batch(steps)\n",
    "print('Overlap within', overlap)\n",
    "for ex in examples.take(1):\n",
    "    art_ex = ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpm = LoudnessPredictorModel()\n",
    "(worst_index, worst_error, worst_example) = plot_pred(lpm, examples)\n",
    "print('worst error at', worst_index, worst_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "noise = make_noise(1, 980, 1020)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(np.arange(len(noise)), noise)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim((0,100))\n",
    "plt.plot(np.arange(len(noise)//2), np.abs(np.fft.fft(noise))[:len(noise)//2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "window_size = 2048\n",
    "ex = synthesize_example([980,1020], 40, [[-30,980, 1020], [-20, 540, 600]], -70, window_size=window_size)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(np.arange(window_size), ex[:window_size,0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(np.arange(window_size//2), np.abs(np.fft.fft(ex[:window_size, 0]))[:window_size//2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "one_khz_examples = load_examples('/home/zond/DriveFileStream/My Drive/korvapuusti/listening_tests/modern_format/by_probe_center/1kHz/*/*.json').batch(512)\n",
    "for ex in one_khz_examples.take(1):\n",
    "    ex0 = ex\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(np.arange(window_size), np.abs(ex0[0][0,:window_size]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertequal(list(itertools.combinations(np.arange(5),4)), [[0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz_cell = PZCell(tf.constant(1.34, dtype=tf.complex128),\n",
    "                 tf.constant([\n",
    "                     (-0.05429768147702485+1.4172655611120915e-05j),\n",
    "                     (0.2917622403739163+0.7731812636894612j),\n",
    "                     (0.8768382244780407-0.31120458350060115j),\n",
    "                     (0.6598943546882394-0.46728573398560225j)]),\n",
    "                 tf.constant([\n",
    "                     (0.635496172349615+0.14499945287904842j),\n",
    "                     (0.30987058966944614-0.8574194617385421j),\n",
    "                     (0.5721096307971768-2.2915816453724273e-05j)]))\n",
    "inputs = tf.ones([2,1])\n",
    "state = tuple(tf.zeros(shape=[inputs.shape[0], size]) for size in pz_cell.state_size)\n",
    "inputs, state = pz_cell.call(inputs, state)\n",
    "inputs, state = pz_cell.call(inputs, state)\n",
    "inputs, state = pz_cell.call(inputs, state)\n",
    "inputs, state = pz_cell.call(inputs, state)\n",
    "inputs, state = pz_cell.call(inputs, state)\n",
    "pz_cell.call(inputs, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertequal(coeffs(tf.constant([\n",
    "    (-0.05429768147702485+1.4172655611120915e-05j),\n",
    "    (0.2917622403739163+0.7731812636894612j),\n",
    "    (0.8768382244780407-0.31120458350060115j),\n",
    "    (0.6598943546882394-0.46728573398560225j),\n",
    "    (-0.05429768147702485-1.4172655611120915e-05j),\n",
    "    (0.2917622403739163-0.7731812636894612j),\n",
    "    (0.8768382244780407+0.31120458350060115j),\n",
    "    (0.6598943546882394+0.46728573398560225j)]), length=8), tf.constant([\n",
    "    (1+0j), \n",
    "    (-3.548394276126343+5.551115123125783e-17j), \n",
    "    (5.916185211745642-3.885780586188048e-16j), \n",
    "    (-5.950823836910118-2.7755575615628914e-17j), \n",
    "    (3.819250686844303+3.3306690738754696e-16j),\n",
    "    (-1.4222617112159746+8.326672684688674e-17j), \n",
    "    (0.1942351079538039-6.938893903907228e-18j), \n",
    "    (0.03639413827830768-2.8189256484623115e-18j), \n",
    "    (0.0011396243277688343+1.0842021724855044e-19j)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_layer = EarLayer()\n",
    "impulse = np.zeros([3, 10, 1], dtype=np.float64)\n",
    "impulse[:,0,:] = 1\n",
    "got = ear_layer(impulse)\n",
    "want_per_batch = [[[0.0], [0.0], [1.34], [0.6880115972617817], [0.5898481920156424], [0.2653899964171016], [-0.1783844973373454], [-0.5351619597902066], [-0.6439493737887534], [-0.5374802978821727]]]\n",
    "want = tf.constant(np.concatenate([want_per_batch, want_per_batch, want_per_batch], axis=0))\n",
    "assertequal(got, want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pz(EarLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_layer = EarLayer()\n",
    "impulse = np.zeros([3, 2048, 1], dtype=np.float64)\n",
    "impulse[:,0,:] = 1\n",
    "t = time.time()\n",
    "got = ear_layer(impulse)\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "got = ear_layer(impulse)\n",
    "print(time.time() - t)\n",
    "plot_many_z(np.array([np.fft.fft(got[0,:,0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_layer = CARLayer(linear=True)\n",
    "impulse = np.zeros([3, 2048, 1], dtype=np.float64)\n",
    "impulse[:,0,:] = 1\n",
    "impulse = tf.constant(impulse, name='impulse')\n",
    "t = time.time()\n",
    "got = car_layer(impulse)\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "got = car_layer(impulse)\n",
    "print(time.time() - t)\n",
    "got = tf.transpose(got, [0,2,1])\n",
    "plot_many_z(np.fft.fft(got[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_layer = CARLayer(erb_per_step=1)\n",
    "impulse = np.zeros([3, 2048, 1], dtype=np.float64)\n",
    "impulse[:,0,:] = 1\n",
    "impulse = tf.constant(impulse, name='impulse')\n",
    "got = car_layer(impulse)\n",
    "got = tf.transpose(got, [0,2,1])\n",
    "plot_many_z(np.fft.fft(got[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrm = SNRModel(linear=True)\n",
    "got = snrm(ex0[0])\n",
    "plot_snr(got[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrm = SNRModel(linear=False)\n",
    "got = snrm(ex0[0])\n",
    "plot_snr(got[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-endorsement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lpm = LoudnessPredictorModel()\n",
    "(worst_index, worst_error, worst_example) = plot_pred(lpm, one_khz_examples)\n",
    "print('worst error at', worst_index, worst_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.tight_layout()\n",
    "plt.xlim((0,150))\n",
    "plt.ylim((-60,50))\n",
    "plt.plot(np.arange(window_size//2), 20 * np.log10(1e-14 + np.abs(np.fft.fft(ex0[0][307][:,0]))[:window_size//2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpm = LoudnessPredictorModel()\n",
    "impulse = np.zeros([3, 2048, 1], dtype=np.float64)\n",
    "impulse[:,0,:] = 1\n",
    "impulse = tf.constant(impulse, name='impulse')\n",
    "lpm(impulse)\n",
    "tape = tf.GradientTape()\n",
    "with tape:\n",
    "    res = lpm(impulse)\n",
    "tape.gradient(res, [\n",
    "    lpm.loudness_predictor_layer._loudness_scale,\n",
    "    lpm.loudness_predictor_layer._loudness_offset,\n",
    "    lpm.car_layer.cc._high_f_damping_compression,\n",
    "    lpm.car_layer.cc._zero_ratio,\n",
    "    lpm.car_layer.cc._min_zeta,\n",
    "    lpm.car_layer.cc._erb_constant_0,\n",
    "    lpm.car_layer.cc._velocity_scale,\n",
    "    lpm.car_layer.cc._v_offset,\n",
    "    lpm.ear_layer.gain,\n",
    "    lpm.ear_layer.poles,\n",
    "    lpm.ear_layer.zeros\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
